{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f71009a",
   "metadata": {},
   "source": [
    "# Student ID, name of each team member\n",
    "107021115 錢珈鋒\n",
    "110065536 陳俊元\n",
    "110062645 王昱文\n",
    "\n",
    "# How did you preprocess data, e.g. cleaning, feature engineering, etc?\n",
    "First we use the idea TA share on the notebook but the result not even touch the baseline,there fore we start to extract as mush features as we can,such as title,topic,channel,adj in the content,the length of the content,date,title,author's name,after that we try all the combination we can possibly image.Finally, we find out the combination of the number of the positive an author get ,date and topic are three of the best combination.\n",
    "\n",
    "# How did you build the classifier, e.g. model, training algorithm, special techniques, etc?\n",
    "\n",
    "Other than the feature, we have googled the sentiment analysis techniques,based on the study we have applied LDA,LLE and all sorts of the distribution and it rather than helping us getting the higger score than lower the socre which was a crush on us.Because all of the research told us that LDA LLE distribution will definitely help us a lot which was a oppositie of the outcome.We still wanna know how to get better score through LDA.\n",
    "\n",
    "# Conclusions, including interesting findings, pitfalls, takeaway lessons, etc.\n",
    "\n",
    "We have learned a lot by using beautifulsoup,we now have the knowledge of extracting the feature we are want.At the first time we upload our prediction it seems great on validation data but terrible on testing data.There's still a big progress we need to try because some of the classmate can reach 0.6 and even better without using DL skills, really looking forward to learn how they did it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf312500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/yuan65536/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yuan65536/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yuan65536/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from sklearn import neighbors\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a3d0c",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9709b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df.head(5)\n",
    "\n",
    "df_test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a467029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use bf4 to convert html\n",
    "def preprocessor(text):\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    \n",
    "    # convert to lowercase and append all emoticons behind (with space in between)\n",
    "    # replace('-','') removes nose of emoticons\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be8a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "def tokenizer_stem_nostop(text):\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in re.split('\\s+', text.strip()) \\\n",
    "            if w not in stop and re.match('[a-zA-Z]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef984125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points:  27643\n"
     ]
    }
   ],
   "source": [
    "x = df['Page content'].values\n",
    "y = df['Popularity'].values\n",
    "print('training data points: ', x.shape[0])\n",
    "\n",
    "x_test = df_test['Page content'].values\n",
    "ID_test = df_test['Id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744eddbd",
   "metadata": {},
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x, y, test_size = 0.2, random_state = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99791f13",
   "metadata": {},
   "source": [
    "# Features extract\n",
    "content length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5820c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countbodynumber(text):\n",
    "    count = 0\n",
    "    for i in text.split():\n",
    "        count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "191dfd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    tag = soup.find('time')\n",
    "    ch = str(tag)\n",
    "    ch = ch.split(' ')\n",
    "    ch = ch[1].split('\"')\n",
    "    ch = ch[1].split(',')\n",
    "    return ch[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897ef056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    tag = soup.find('article')\n",
    "    ch = str(tag)\n",
    "    ch = ch.split('>')\n",
    "    ch = ch[0].split(' ')\n",
    "    ch = ch[1].split('\"')\n",
    "    return (ch[1])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc0e04",
   "metadata": {},
   "source": [
    "extract title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecadddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettitle(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    title_tag = soup.find(class_ = 'title')\n",
    "    \n",
    "    return title_tag.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c2292",
   "metadata": {},
   "source": [
    "extract author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7413c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthorName(text):\n",
    "    text = text.replace(\"</a>\", \" \")\n",
    "    text = text.replace(\"by\", \"\")\n",
    "    text = text.replace(\"By\", \"\")\n",
    "    # remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # regex for matching emoticons, keep emoticons, ex: :), :-P, :-D\n",
    "    r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "    emoticons = re.findall(r, text)\n",
    "    text = re.sub(r, '', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-','')\n",
    "    count = 0\n",
    "    l = []\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == \" \":\n",
    "            if count == 2:\n",
    "                break\n",
    "            if count == 1:\n",
    "                l.append(\" \") \n",
    "                count = 2\n",
    "            \n",
    "        if text[i] != \" \":\n",
    "            if count == 0:\n",
    "                count = 1\n",
    "            l.append(text[i])\n",
    "\n",
    "    s = ''.join(l)\n",
    "    return s\n",
    "\n",
    "def getAuthorList(X):\n",
    "    Name = []\n",
    "    for i in range(len(X)):\n",
    "        name = getAuthorName(X[i])\n",
    "        Name.append(name)\n",
    "    return Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79aeb5e",
   "metadata": {},
   "source": [
    "get part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "721ae279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(title):\n",
    "    tokens = nltk.word_tokenize(title)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    temp = []\n",
    "    for word, pos in pos_tags:\n",
    "        if (pos == 'JJ' or pos == 'JJR' or pos == 'JJS'):\n",
    "            temp.append(word)\n",
    "    \n",
    "    pos_str = ' '.join(temp)\n",
    "    \n",
    "    return pos_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa3f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAuthorCount(AuthorCountLabelList,NameList):\n",
    "    Name=AuthorCountLabelList[:,0].tolist()\n",
    "    NameCount=AuthorCountLabelList[:,1].tolist()\n",
    "    AuthorCountList=[]\n",
    "    for i in range(len(NameList)):\n",
    "        if NameList[i]  in Name:\n",
    "            index=Name.index(NameList[i])\n",
    "            AuthorCountList.append(float(NameCount[index]))\n",
    "        else:\n",
    "            AuthorCountList.append(0)\n",
    "    return AuthorCountList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e5f85",
   "metadata": {},
   "source": [
    "get date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492aa0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_date(text):\\n    soup = BeautifulSoup(text, 'html.parser')\\n    date_tag = soup.find('time')\\n    \\n    return date_tag.text\\n\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_date(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    date_tag = soup.find('time')\n",
    "    \n",
    "    return date_tag.text\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebd94140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    topics = [str(i.string) for i in soup.find_all('a') if i.parent.name == 'footer']\n",
    "    topics = \" \".join(topics)\n",
    "\n",
    "    return topics\n",
    "\n",
    "def getTopicsList(X):\n",
    "    Topics=[]\n",
    "    for i in range(len(X)):\n",
    "        topics=getTopics(X[i])\n",
    "        Topics.append(topics)\n",
    "    return Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f22981fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    topic_list = []\n",
    "    pos = []\n",
    "    content_length = []\n",
    "    author_list = getAuthorList(data)\n",
    "    channel=[]\n",
    "    for i in range(len(data)):\n",
    "        title_tmp = gettitle(data[i])\n",
    "        title_list.append(title_tmp)\n",
    "        \n",
    "        word = get_part_of_speech(title_tmp)\n",
    "        pos.append(word)\n",
    "        \n",
    "        \n",
    "        ch = get_channel(data[i])\n",
    "        channel.append(ch)\n",
    "        \n",
    "        \n",
    "        text = preprocessor(data[i])\n",
    "        content_length.append(countbodynumber(text))\n",
    "        \n",
    "        date_tmp = get_date(str(data[i]))\n",
    "        if(date_tmp=='='):date_list.append('Mon')\n",
    "        else:date_list.append(date_tmp)\n",
    "        \n",
    "        topic_list.append(get_topic(data[i]))\n",
    "        \n",
    "        \n",
    "    return title_list, author_list, pos, content_length, date_list, topic_list,channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4539f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAuthorCountLabel(NameList):\n",
    "    Name=[]\n",
    "    NameCount=[]\n",
    "    for j in range(len(NameList)):\n",
    "        if NameList[j] not in Name:\n",
    "            Name.append(NameList[j])\n",
    "            NameCount.append(1)\n",
    "        else :\n",
    "            index=Name.index(NameList[j])\n",
    "            temp=NameCount[index]\n",
    "            NameCount[index]=temp+1\n",
    "    Name_np = np.array(Name)\n",
    "    NameCount_np = np.array(NameCount)\n",
    "    AuthorCountLebalList = np.concatenate((Name_np.reshape(-1,1),NameCount_np.reshape(-1,1)), axis=1)\n",
    "    return AuthorCountLebalList\n",
    "\n",
    "def getAuthorCount(AuthorCountLabelList,NameList):\n",
    "    Name=AuthorCountLabelList[:,0].tolist()\n",
    "    NameCount=AuthorCountLabelList[:,1].tolist()\n",
    "    AuthorCountList=[]\n",
    "    for i in range(len(NameList)):\n",
    "        if NameList[i]  in Name:\n",
    "            index=Name.index(NameList[i])\n",
    "            AuthorCountList.append(float(NameCount[index]))\n",
    "        else:\n",
    "            AuthorCountList.append(0)\n",
    "    return AuthorCountList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e53cab",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39afbbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntitle_train, authorf_train, posf_train, content_lengthf_train, datef_train, topic_train,channel_train = get_features(x_train)\\ntitle_valid, authorf_valid, posf_valid, content_lengthf_valid, datef_valid, topic_valid,channel_valid = get_features(x_valid)\\n\\n\\nNameList_1 = []\\nNameList_2 = []\\n\\nAuthorCountLabelList_t=buildAuthorCountLabel(NameList_1)\\nAuthorCountList_t=getAuthorCount(AuthorCountLabelList_t,authorf_train)\\n\\nAuthorCountLabelList_v=buildAuthorCountLabel(NameList_2)\\nAuthorCountList_v=getAuthorCount(AuthorCountLabelList_v,authorf_valid)\\n\\n\\n\\ntt = np.array(title_train)\\nat = np.array(authorf_train)\\npt = np.array(posf_train)\\ndt = np.array(datef_train)\\nto_t = np.array(topic_train)\\nct = np.array(channel_train)\\nac_t = np.array(AuthorCountList_t)\\n\\ntv = np.array(title_valid)\\nav = np.array(authorf_valid)\\npv = np.array(posf_valid)\\ndv = np.array(datef_valid)\\nto_v = np.array(topic_valid)\\ncv = np.array(channel_valid)\\nac_v = np.array(AuthorCountList_v)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "title_train, authorf_train, posf_train, content_lengthf_train, datef_train, topic_train,channel_train = get_features(x_train)\n",
    "title_valid, authorf_valid, posf_valid, content_lengthf_valid, datef_valid, topic_valid,channel_valid = get_features(x_valid)\n",
    "\n",
    "\n",
    "NameList_1 = []\n",
    "NameList_2 = []\n",
    "\n",
    "AuthorCountLabelList_t=buildAuthorCountLabel(NameList_1)\n",
    "AuthorCountList_t=getAuthorCount(AuthorCountLabelList_t,authorf_train)\n",
    "\n",
    "AuthorCountLabelList_v=buildAuthorCountLabel(NameList_2)\n",
    "AuthorCountList_v=getAuthorCount(AuthorCountLabelList_v,authorf_valid)\n",
    "\n",
    "\n",
    "\n",
    "tt = np.array(title_train)\n",
    "at = np.array(authorf_train)\n",
    "pt = np.array(posf_train)\n",
    "dt = np.array(datef_train)\n",
    "to_t = np.array(topic_train)\n",
    "ct = np.array(channel_train)\n",
    "ac_t = np.array(AuthorCountList_t)\n",
    "\n",
    "tv = np.array(title_valid)\n",
    "av = np.array(authorf_valid)\n",
    "pv = np.array(posf_valid)\n",
    "dv = np.array(datef_valid)\n",
    "to_v = np.array(topic_valid)\n",
    "cv = np.array(channel_valid)\n",
    "ac_v = np.array(AuthorCountList_v)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f54275cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_train, authorf_train, posf_train, content_lengthf_train, datef_train, topic_train,channel_train = get_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c2b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameList=getAuthorList(x)\n",
    "NameList_1 = []\n",
    "j=0\n",
    "for i in range(len(NameList)):\n",
    "    if y[i]==1:\n",
    "        NameList_1.append(NameList[i])\n",
    "        j=j+1\n",
    "    else:\n",
    "        NameList_1.append(NameList[i])\n",
    "        \n",
    "AuthorCountLabelList=buildAuthorCountLabel(NameList_1)\n",
    "AuthorCountList=getAuthorCount(AuthorCountLabelList,authorf_train)\n",
    "AuthorCountList = np.array(AuthorCountList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "307e78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datef_train=np.array(datef_train)\n",
    "topic_train=np.array(topic_train)\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**13,\n",
    "                            preprocessor=preprocessor,\n",
    "                            tokenizer=tokenizer_stem_nostop)\n",
    "\n",
    "onehot_date = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "\n",
    "Date_onehot_train = onehot_date.fit_transform(datef_train.reshape(-1, 1))\n",
    "Topic_hash_train = hashvec.transform(topic_train)\n",
    "\n",
    "\n",
    "X = np.concatenate((Date_onehot_train.toarray(), Topic_hash_train.toarray(),AuthorCountList.reshape(-1,1)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6bbb1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27643\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "NameList = np.array(NameList)\n",
    "\n",
    "print(len(NameList))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d641325",
   "metadata": {},
   "source": [
    "clt = np.array(content_lengthf_train)\n",
    "clv = np.array(content_lengthf_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33e8d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(tag_t, tag_v):\n",
    "    count = CountVectorizer(ngram_range = (1, 1),\n",
    "                            preprocessor = preprocessor,\n",
    "                            tokenizer = tokenizer_stem_nostop)\n",
    "    \n",
    "    res_t = count.fit_transform(tag_t)\n",
    "    res_v = count.transform(tag_v)\n",
    "    \n",
    "    return res_t, res_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77e0e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_fun(tag_t, tag_v, fea):\n",
    "    hash_f = HashingVectorizer(n_features = fea,\n",
    "                                   preprocessor = preprocessor,\n",
    "                                   tokenizer = tokenizer_stem_nostop)\n",
    "    \n",
    "    res_t = hash_f.fit_transform(tag_t)\n",
    "    res_v = hash_f.transform(tag_v)\n",
    "    \n",
    "    return res_t, res_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a4ddb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(tag_t, tag_v):\n",
    "    onehot = OneHotEncoder(handle_unknown = 'ignore')\n",
    "    \n",
    "    res_t = onehot.fit_transform(tag_t.reshape(-1, 1))\n",
    "    res_v = onehot._transform(tag_v.reshape(-1, 1))\n",
    "    \n",
    "    return res_t, res_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64bd8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_fun(tag_t, tag_v):\n",
    "    tfidf = TfidfVectorizer(ngram_range = (1,1),\n",
    "                            preprocessor = preprocessor,\n",
    "                            tokenizer = tokenizer_stem_nostop)\n",
    "    \n",
    "    res_t = tfidf.fit_transform(tag_t)\n",
    "    res_v = tfidf.transform(tag_v)\n",
    "    \n",
    "    return res_t, res_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6dcf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_fun(t, v, n):\n",
    "    lda = LatentDirichletAllocation(n_components = n, random_state = 42)\n",
    "    \n",
    "    res_t = lda.fit_transform(t)\n",
    "    res_v = lda.transform(v)\n",
    "    \n",
    "    return res_t, res_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b223159a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#t, t2 = bow(tt, tv)\\nt,t2  = hash_fun(tt,tv,2**10)   #title\\na, a2 = hash_fun(at, av, 2**11) #author\\np, p2 = bow(pt, pv)             #pos\\nd, d2 = ohe(dt, dv)     #date\\n\\n\\nto, to2 = hash_fun(to_t, to_v, 2**10) #topic\\n\\nct_t, cv_v = hash_fun(ct, cv, 2**10) #channel'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#t, t2 = bow(tt, tv)\n",
    "t,t2  = hash_fun(tt,tv,2**10)   #title\n",
    "a, a2 = hash_fun(at, av, 2**11) #author\n",
    "p, p2 = bow(pt, pv)             #pos\n",
    "d, d2 = ohe(dt, dv)     #date\n",
    "\n",
    "\n",
    "to, to2 = hash_fun(to_t, to_v, 2**10) #topic\n",
    "\n",
    "ct_t, cv_v = hash_fun(ct, cv, 2**10) #channel'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8827c5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title_t = t.toarray()\\nauthor_t = a.toarray()\\npos_t = p.toarray()\\ndate_t = d.toarray()\\ntopic_t = to.toarray()\\nct_t = ct_t.toarray()\\nAcount_t=ac_t.reshape(-1,1)\\n#doc_train_temp = np.array(doc_train)\\n#train_tfidf = doc_train_temp.toarray()\\n\\n\\n\\ntitle_v = t2.toarray()\\nauthor_v = a2.toarray()\\npos_v = p2.toarray()\\ndate_v = d2.toarray()\\ntopic_v = to2.toarray()\\ncv_v = cv_v.toarray()\\nAcount_v=ac_v.reshape(-1,1)\\n\\n#doc_valid_temp = np.array(doc_valid)\\n#valid_tfidf = doc_valid_temp.toarray()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''title_t = t.toarray()\n",
    "author_t = a.toarray()\n",
    "pos_t = p.toarray()\n",
    "date_t = d.toarray()\n",
    "topic_t = to.toarray()\n",
    "ct_t = ct_t.toarray()\n",
    "Acount_t=ac_t.reshape(-1,1)\n",
    "#doc_train_temp = np.array(doc_train)\n",
    "#train_tfidf = doc_train_temp.toarray()\n",
    "\n",
    "\n",
    "\n",
    "title_v = t2.toarray()\n",
    "author_v = a2.toarray()\n",
    "pos_v = p2.toarray()\n",
    "date_v = d2.toarray()\n",
    "topic_v = to2.toarray()\n",
    "cv_v = cv_v.toarray()\n",
    "Acount_v=ac_v.reshape(-1,1)\n",
    "\n",
    "#doc_valid_temp = np.array(doc_valid)\n",
    "#valid_tfidf = doc_valid_temp.toarray()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fdfa496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_t_feature = np.concatenate((date_t,topic_t,Acount_t), axis = 1)\\nx_v_feature = np.concatenate((date_v,topic_v,Acount_v), axis = 1)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''x_t_feature = np.concatenate((date_t,topic_t,Acount_t), axis = 1)\n",
    "x_v_feature = np.concatenate((date_v,topic_v,Acount_v), axis = 1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecddf913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train roc: 0.624164, valid roc: 0.625737\n",
      "train acc: 0.624651, valid roc: 0.626553\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "lr = LogisticRegression( solver = \"liblinear\")\n",
    "lr.fit(X, y)\n",
    "\n",
    "y_train_pred = lr.predict(x_train)\n",
    "y_valid_pred = lr.predict(x_valid)\n",
    "\n",
    "\n",
    "print('train roc: %f, valid roc: %f' % (roc_auc_score(y_train, y_train_pred),\n",
    "                                        roc_auc_score(y_valid, y_valid_pred)))\n",
    "    \n",
    "print('train acc: %f, valid roc: %f' % (accuracy_score(y_train, y_train_pred),\n",
    "                                        accuracy_score(y_valid, y_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c626c53",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a793d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_test, authorf_test, posf_test, content_lengthf_test, datef_test, topic_test,channel_test = get_features(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "243b15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NameList_test=getAuthorList(x_test)\n",
    "AuthorCountList_t=getAuthorCount(AuthorCountLabelList,NameList_test)\n",
    "\n",
    "\n",
    "datef_test=np.array(datef_test)\n",
    "topic_test=np.array(topic_test)\n",
    "\n",
    "\n",
    "\n",
    "AuthorCountList_test = np.array(AuthorCountList_t)\n",
    "Date_onehot_test = onehot_date.transform(datef_test.reshape(-1, 1))\n",
    "Topic_hash_test  = hashvec.transform(topic_test)\n",
    "\n",
    "\n",
    "test = np.concatenate((Date_onehot_test.toarray(), Topic_hash_test.toarray(),AuthorCountList_test.reshape(-1,1)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(topic_test)\n",
    "#print(e_test.toarray())\n",
    "#print(AuthorCountList_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e714504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#import numpy\n",
    "#numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#print(AuthorCountList_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a6d99",
   "metadata": {},
   "source": [
    "ttt = np.array(title_test)\n",
    "att = np.array(authorf_test)\n",
    "ptt = np.array(posf_test)\n",
    "dtt = np.array(datef_test)\n",
    "to_tt = np.array(topic_test)\n",
    "cltt = np.array(content_lengthf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cf579",
   "metadata": {},
   "source": [
    "t, t2 = bow(tt, ttt)\n",
    "a, a2 = hash_fun(at, att, 2**11) #2**11\n",
    "p, p2 = bow(pt, ptt)\n",
    "content_length_tt = cltt.reshape(len(x_test), 1)\n",
    "d, d2 = bow(dt, dtt)\n",
    "to, to2 = hash_fun(to_t, to_tt, 2**10) #2**10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a72183",
   "metadata": {},
   "source": [
    "title_tt = t2.toarray()\n",
    "author_tt = a2.toarray()\n",
    "pos_tt = p2.toarray()\n",
    "date_tt = d2.toarray()\n",
    "topic_tt = to2.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a36334",
   "metadata": {},
   "source": [
    "x_tt_feature = np.concatenate((author_tt, content_length_tt, date_tt, topic_tt), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc00cce",
   "metadata": {},
   "source": [
    "# Write csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1520faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lr.predict(test)\n",
    "d = {'Id': ID_test,'Popularity': result}\n",
    "output_test_pred = pd.DataFrame(data = d)\n",
    "output_test_pred.to_csv('y_predict.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "969f08a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Id  Popularity\n",
      "0      27643          -1\n",
      "1      27644           1\n",
      "2      27645          -1\n",
      "3      27646          -1\n",
      "4      27647          -1\n",
      "...      ...         ...\n",
      "11842  39485           1\n",
      "11843  39486          -1\n",
      "11844  39487           1\n",
      "11845  39488           1\n",
      "11846  39489          -1\n",
      "\n",
      "[11847 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(output_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecb9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
